{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BFW3BEl-2e4"
      },
      "source": [
        "# Emotion Detection con tecniche di Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OofHmTy1xMB"
      },
      "source": [
        "## Import delle librerie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMErFc4d16jC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Input, SimpleRNN, LSTM, Dense, Dropout, BatchNormalization, Flatten, Conv1D, Activation, MaxPool1D\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import wave #used to calculate audio file length\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio #used for cleaning too long cells output, used to play tracks\n",
        "import librosa\n",
        "import librosa.display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_dataset = False #work with augmented dataset or with the default one\n",
        "\n",
        "dataset_path = \"Audio_Speech_Actors_01-24\"\n",
        "dataframe_path = \"dataframe.csv\"\n",
        "dataframe_aug_path = \"dataframe-aug.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHPu6WwPj36g"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4bejdoUj5qq"
      },
      "outputs": [],
      "source": [
        "def plot_history(loss_values, accuracy_values, plot_file_name=\"plot\"):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "  ax1.set_title(\"Accuracy and loss trend\")\n",
        "  \n",
        "  #get number of cross-validation training iteration\n",
        "  iteration_count = len(loss_values)\n",
        "  #print x axis label\n",
        "  ax1.set_xlabel('Epochs') \n",
        "\n",
        "  #print loss trend\n",
        "  line1, = ax1.plot(range(1,iteration_count+1),loss_values,label='loss',color='orange')\n",
        "  #show y axis label and color axis values\n",
        "  ax1.set_ylabel('loss',color = line1.get_color())\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  #show legend\n",
        "  _ = ax1.legend(loc='lower left')\n",
        "\n",
        "  #clone y axis\n",
        "  ax2 = ax1.twinx()\n",
        "  #print accuracy trend\n",
        "  line2, = ax2.plot(range(1,iteration_count+1),accuracy_values,label='accuracy')\n",
        "  #show y axis label and color axis values\n",
        "  ax2.set_ylabel('accuracy',color = line2.get_color())\n",
        "  ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "  #show legend\n",
        "  _=ax2.legend(loc='upper right')\n",
        "\n",
        "  fig.savefig(\"{}.png\".format(plot_file_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNyAzNj3J0rT"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr9DdWNA6Xnt"
      },
      "source": [
        "Il dataset è una sottoporzione del dataset RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song). Mentre il RAVDESS puro contiene 24.8GB di discorsi e di canzoni, audio e video, il RAVDESS Emotional speech audio contiene solo audio di discorsi.\n",
        "\n",
        "Il dataset contiene 60 tracce audio per ognuno dei 24 attori, per un totale di 1440 file in formato war. Gli attori (12 maschi e 12 femmine) pronunciano 2 frasi (2 volte ciascuna) con un accento neutro del Norm America esprimendo emozioni differenti (calma, felicità, tristezza, rabbia, paura, sorpresa e disgusto) a 2 differenti livelli di intensità (medio, alto) più un'espressione neutra.\n",
        "\n",
        "La **naming convention** dei file del dataset è la seguente: ognuno dei 1440 file è identificato da una successione di 7 identificatori numerici (es. 03-01-06-01-02-01-12.wav) e ognuno di questi numeri ha un significato specifico:\n",
        "\n",
        "* **Modalità** (01: audio-video, 02: solo video, 03: solo audio)\n",
        "  * in questa sottoporzione del dataset la modalità è sempre audio-only\n",
        "* **Canale vocale** (01: parlato, 02: cantato)\n",
        "  * in questa sottoporzione del dataset il canale vocale è sempre parlato \n",
        "* **Emozione** (01: neutra, 02: calma, 03: felicità, 04: tristezza, 05: rabbia, 06: paura, 07: disgusto, 08: sorpresa)\n",
        "* **Intensità** (01: normale, 02: forte)\n",
        "  * per l'emozione neutra non c'è intensità\n",
        "* **Frase** (01: \"Kids are talking by the door\", 02: \"Dogs are sitting by the door\")\n",
        "* **Ripetizione** (01: prima ripetizione, 02: seconda ripetizione)\n",
        "* **Attore** (01...24, gli attori dispari sono maschi, mentre quelli pari sono femmine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQzdRwmiasEJ"
      },
      "source": [
        "### Funzione di campionamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phdG1gd-DSxo"
      },
      "source": [
        "Viene definita una utility function `sample_track` che restituisce in output il path di una traccia casuale campionato dal dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRYMFMdgdyQK"
      },
      "outputs": [],
      "source": [
        "def sample_track():\n",
        "  #choose a random folder\n",
        "  folders = sorted(glob.glob(dataset_path+\"/*\"), key=len)\n",
        "  random_folder = np.random.choice(folders)\n",
        "  #choose and return a random track\n",
        "  tracks = sorted(glob.glob(random_folder+\"/*.wav\"), key=len)\n",
        "  random_track = np.random.choice(tracks)\n",
        "  return random_track  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "Dtdxay45fw9P",
        "outputId": "c0b7fd87-170d-4315-e050-c081e346f06f"
      },
      "outputs": [],
      "source": [
        "#sampling function testing \n",
        "random_track = sample_track()\n",
        "#track visualization\n",
        "Audio(random_track)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urf9NMJdHyYh"
      },
      "source": [
        "### Caricamento dei file audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whxl_zGfCkZi"
      },
      "source": [
        "La funzione [load](https://librosa.org/doc/main/generated/librosa.load.html) di librosa permette di caricare un file audio come una sequenza temporale di valori floating point.\n",
        "\n",
        "* `path` indica alla funzione il path del file caricare\n",
        "\n",
        "La funzione restituisce la sequenza di valori floating point e la frequenza di campionatura (espressa in Hz). Il valore di default per la frequenza di campionatura è di 22.050 Hz, il che significa che una traccia audio di 3 secondi viene caricata come un vettore di 66.150 valori floatin point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxP3wccqhRsJ",
        "outputId": "9387fde9-f5ef-4b00-b908-3e8d12b99770"
      },
      "outputs": [],
      "source": [
        "#load the audio track as a floating point time series\n",
        "time_series, sampling_rate = librosa.load(path=random_track)\n",
        "print(\"sampling_rate: {}\".format(sampling_rate))\n",
        "print(\"time_series length: {}\".format(len(time_series)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgTYld9rKMXE"
      },
      "source": [
        "la funzione `get_audio_length` restituisce la durata in secondi della traccia audio passata in input. Usa il package [wave](https://docs.python.org/3/library/wave.html) per estrarre l'informazione dai file WAV.\n",
        "\n",
        "* `path` è il percorso della traccia audio\n",
        "\n",
        "La funzione viene usata per verificare la lunghezza della serie temporale estratta da librosa, basterà infatti moltiplicare la lunghezza del file per il rateo di campionatura e controllare se il valore coincide con il numero di valori estratti da librosa. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbMhWXufIdle"
      },
      "outputs": [],
      "source": [
        "def get_audio_length(path):\n",
        "    file = wave.open(path, 'r')\n",
        "    frames = file.getnframes()\n",
        "    #print(\"frames: \", frames)\n",
        "    rate = file.getframerate()\n",
        "    #print(\"frame rate: \", rate)\n",
        "    return frames / float(rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHcuOqJpI2fJ",
        "outputId": "f3039688-9c6c-4970-9967-d55457b8f5c0"
      },
      "outputs": [],
      "source": [
        "audio_length = get_audio_length(random_track)\n",
        "print(\"audio length: {}\".format(audio_length))\n",
        "print(\"expected time series length: {}\".format(math.ceil(audio_length * sampling_rate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "nlCVUnGrHoNv",
        "outputId": "9ee0737f-7f71-4aa9-b9a6-12a457723815"
      },
      "outputs": [],
      "source": [
        "#display track spectrogram\n",
        "spectrogram = librosa.feature.melspectrogram(y=time_series, sr=sampling_rate)\n",
        "librosa.display.waveshow(y=spectrogram, sr=sampling_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKKzO4oazb9j"
      },
      "source": [
        "### Visualizzazione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwO39N4b2zzE"
      },
      "source": [
        "Per fini di visualizzazione viene costruito un DataFrame Pandas ottenuto dalle informazioni codificate nei nomi dei singoli file. In particolare le colonne saranno le seguenti: `Path`, `Emotion`, `Intensity`, `Statement`, `Actor`, `Gender`.\n",
        "\n",
        "Prima vengono generati array di valori in base ai codici contenuti nei nomi dei file, poi quegli array vengono convertiti in dataframe con una singola colonna e infine i dataframes vengono concatenati per comporre il dataframe obiettivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Nuy0UCyNzeMW",
        "outputId": "e5df75a7-cb96-4502-c8f3-664aebc049a4"
      },
      "outputs": [],
      "source": [
        "directories = os.listdir(dataset_path)\n",
        "\n",
        "#support arrays to store database values\n",
        "emotion_col = []; intensity_col = []; statement_col = []; actor_col = []; gender_col = []; path_col = []\n",
        "\n",
        "#iterate over dataset folders (every folder is relative to a different actor)\n",
        "for dir_name in directories:\n",
        "  dir_files = os.listdir(dataset_path + \"/\" + dir_name)\n",
        "  #iterate over files in the current directory\n",
        "  for file_name in dir_files:\n",
        "    #file name splitting: first remove WAV extension then split over '-' chars\n",
        "    file_parts = file_name.split('.')[0].split('-')\n",
        "    #store each value into the relative array\n",
        "    emotion_col.append(int(file_parts[2]))\n",
        "    intensity_col.append(int(file_parts[3]))\n",
        "    statement_col.append(int(file_parts[4]))\n",
        "    actor_col.append(int(file_parts[6]))\n",
        "    gender_col.append(int(file_parts[6])%2) # %2 to keep only odd/even information\n",
        "    path_col.append(dataset_path + \"/\" + dir_name + \"/\" + file_name)\n",
        "\n",
        "#convert arrays to dataframes and chain them in a single dataframe\n",
        "emotion_df = pd.DataFrame(emotion_col, columns=['Emotion'])\n",
        "intensity_df = pd.DataFrame(intensity_col, columns=['Intensity'])\n",
        "statement_df = pd.DataFrame(statement_col, columns=['Statement'])\n",
        "actor_df = pd.DataFrame(actor_col, columns=['Actor'])\n",
        "gender_df = pd.DataFrame(gender_col, columns=['Gender'])\n",
        "path_df = pd.DataFrame(path_col, columns=['Path'])\n",
        "\n",
        "#convert integer in dataframe with their value following the naming convention\n",
        "#  inplace param added to edit the current dataframe instead of creating a new one\n",
        "emotion_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fearful', 7:'disgust', 8:'surprised'}, inplace=True)\n",
        "intensity_df.replace({1:'normal', 2:'strong'}, inplace=True)\n",
        "statement_df.replace({1:'Kids are talking by the door', 2:'Dogs are sitting by the door'}, inplace=True)\n",
        "gender_df.replace({1:'male', 0:'female'}, inplace=True)\n",
        "\n",
        "files_df = pd.concat([path_df, emotion_df, intensity_df, statement_df, actor_df, gender_df], axis=1) #axis=1 to concatenate columns instead of rows\n",
        "files_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifdXm_EzHUBp"
      },
      "source": [
        "Vengono contati i valori assumibili dalla colonna Emotions del dataframe per definire il numero di classi possibili"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSoFlkdLHQms",
        "outputId": "cca4f23c-140b-402e-8087-0605c6c239f9"
      },
      "outputs": [],
      "source": [
        "classes_count = files_df[\"Emotion\"].nunique()\n",
        "print(\"Number of possible classes (emotions): {}\".format(classes_count))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Augmentation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sono state usate tecniche di data augmentation per incrementare la dimensione del dataset. Per la fonte dei metodi si rimanda al notebook [Audio Emotion | Data Augmentation](https://www.kaggle.com/code/ejlok1/audio-emotion-part-5-data-augmentation).\n",
        "\n",
        "Le tecniche usate sono le seguenti:\n",
        "\n",
        "* static noise\n",
        "* shift\n",
        "* stretch\n",
        "* pitch\n",
        "* dynamic change\n",
        "* speed and pitch\n",
        "\n",
        "Nei sottoparagrafi successivi vengono implementati i metodi e testati su un file audio di test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_series, sampling_rate = librosa.load(path=random_track)\n",
        "Audio(time_series, rate=sampling_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Static noise"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il metodo `add_static_noise` aggiunge del rumore di fondo al file passato in input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_static_noise(time_series):\n",
        "    noise_amp = 0.05 * np.random.uniform() * np.amax(time_series)\n",
        "    return time_series.astype('float64') + noise_amp * np.random.normal(size=time_series.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(time_series))\n",
        "noised_time_series = add_static_noise(time_series)\n",
        "print(len(noised_time_series))\n",
        "Audio(noised_time_series, rate=sampling_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shift_audio(time_series):\n",
        "    shift_range = int(np.random.uniform(low=5,high=5) * 1000)\n",
        "    return np.roll(time_series,shift_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shifted_time_series = shift_audio(time_series)\n",
        "print(len(shifted_time_series))\n",
        "Audio(shifted_time_series, rate=sampling_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stretch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stretch_audio(time_series):\n",
        "    return librosa.effects.time_stretch(time_series,rate=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stretched_time_series = stretch_audio(time_series)\n",
        "Audio(stretched_time_series, rate=sampling_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pitch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pitch_audio(time_series, sampling_rate):\n",
        "    bins_per_octave = 12\n",
        "    pitch_pm = 2\n",
        "    pitch_change = pitch_pm * 2 * (np.random.uniform())\n",
        "    return librosa.effects.pitch_shift(y=time_series.astype('float64'),\n",
        "                                       sr=sampling_rate,\n",
        "                                       n_steps=pitch_change,\n",
        "                                       bins_per_octave=bins_per_octave)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pitched_time_series = pitch_audio(time_series, sampling_rate)\n",
        "Audio(pitched_time_series, rate=sampling_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dynamic change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dynamic_change(time_series):\n",
        "    return time_series * np.random.uniform(low=-0.5,high=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "changed_time_series = dynamic_change(time_series)\n",
        "Audio(changed_time_series, rate=sampling_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Speed and pitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def speed_and_pitch(time_series):\n",
        "    length_change = np.random.uniform(low=0.8,high=1)\n",
        "    speed_factor = 1.2 / length_change\n",
        "    tmp = np.interp(np.arange(0,len(time_series),speed_factor), np.arange(0,len(time_series)), time_series)\n",
        "    minlen = min(time_series.shape[0], tmp.shape[0])\n",
        "    time_series *= 0\n",
        "    time_series[0:minlen] = tmp[0:minlen]\n",
        "    return time_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sap_time_series = speed_and_pitch(time_series)\n",
        "Audio(sap_time_series, rate=sampling_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztjb3ctED2Gk"
      },
      "source": [
        "### Features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dal dataset vengono estratte le features usando le funzioni offerte dalla libreria librosa:\n",
        "\n",
        "* [chroma](https://librosa.org/doc/main/generated/librosa.feature.chroma_stft.html): **TODO** (12 valori per ogni file)\n",
        "* [mel spctrogram](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html): spettrogramma dove le frequenze sono state convertine nella scala MEL (128 valori per ogni file)\n",
        "* [mfcc](https://librosa.org/doc/main/generated/librosa.feature.mfcc.html): **TODO** (40 valori per ogni file)\n",
        "* [spectral contrast](https://librosa.org/doc/main/generated/librosa.feature.spectral_contrast.html): **TODO** (7 valori per ogni file)\n",
        "* [tonnetz](https://librosa.org/doc/main/generated/librosa.feature.tonnetz.html): **TODO** (6 valori per ogni file)\n",
        "\n",
        "I valori indicati tra parentesi (12,128,40,7,6 = **193 features**) indicano il numero di colonne di ogni feature, che verranno aggiunte al dataframe Pandas, corredato con l'informazione di classe (corrispondente all'emozione espressa nella traccia audio) del file corrispondente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOOR4oGa66ek"
      },
      "source": [
        "<!-- #### Short-Time Fourier Transform -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBOqe1C57A9H"
      },
      "source": [
        "<!--\n",
        "La **Fourier Transform** (FT) è una funzione matematica in grado di decomporre un segnale nelle sue frequenze costituenti corredate dalle relative magnitudo. La FT permette di passare dal dominio del tempo (time-domain) a quello della frequenza (frequency-domain).\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=12vRYA33koEKGD2fvZOYKUxMBhkXzd3ab\"/>\n",
        "</div>\n",
        "\n",
        "La libreria librosa contiene una funzione per il calcolo della **Discrete Fourier Transform** (DFT), chiamata anche **Fast Fourier Transform** (FFT), funzione matematica che si differenzia dalla FT per il fatto di prendere un input discreto, piuttosto che un input continuo come nella FT standard.\n",
        "\n",
        "La frequenza cambia nel tempo, quindi una FT su tutta la traccia audio può non essere rappresentativa. La **Short-Time Fourier Transform** (STFT) effettua una FT su sottoporzioni della traccia stessa.\n",
        "\n",
        "La STFT viene calcolata usando la funzione [stft](https://librosa.org/doc/main/generated/librosa.stft.html) di librosa, che di norma restituisce un vettore di numeri complessi (float64), che codificano fase e ampiezza del segnale audio, ma la fase, oltre a non venire percepita dall'essere umano, non è utile ai fini dell'esperimento, quindi convertono i valori della sequenza temporale in valori assoluti (float32).\n",
        "\n",
        "**TODO** parlare del binning\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2UByyJ34OyB"
      },
      "source": [
        "#### Estrazione"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vJaF4Ri84a5g"
      },
      "source": [
        "La funzione `get_features` estrae le 193 features dal file passato in input e restituisce 5 array, uno per ogni categoria di feature sopraelencata.\n",
        "\n",
        "* `file_path` è il percorso del file per il quale estrarre le features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-tP2Q0HbJQZ"
      },
      "outputs": [],
      "source": [
        "def get_features(time_series, sampling_rate=22050):\n",
        "  stft = np.abs(librosa.stft(time_series))\n",
        "\n",
        "  chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sampling_rate).T,axis=0)\n",
        "  mel_spectrogram = np.mean(librosa.feature.melspectrogram(y=time_series, sr=sampling_rate).T,axis=0)\n",
        "  mfcc = np.mean(librosa.feature.mfcc(y=time_series, sr=sampling_rate, n_mfcc=40).T,axis=0)\n",
        "  spectral_contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sampling_rate).T,axis=0)\n",
        "  tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(time_series), sr=sampling_rate).T,axis=0)\n",
        "  return chroma, mel_spectrogram, mfcc, spectral_contrast, tonnetz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNbRl9OsG7w7"
      },
      "source": [
        "La funzione `get_features` viene usata per definire quante feature verranno passate in input ai vari modelli."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbMp_NJeGfkz",
        "outputId": "ffc832f7-f785-4d31-b7c1-49a8cd038891"
      },
      "outputs": [],
      "source": [
        "features_count = sum(len(f) for f in get_features(time_series))\n",
        "#features_count = len(get_features(random_track))\n",
        "\n",
        "print(\"features for every file: {}\".format(features_count))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3qzESbnN9mJJ"
      },
      "source": [
        "La funzione `get_feature_dataframe` restituisce un dataframe Pandas contenente le feature estratte da ogni file del dataset. Il dataframe conterrà quindi una riga per ogni file nel dataset (**1440 righe** nella forma standard, **10.080 righe** nella versione estesa applicando la data augmentation) e una colonna per ogni feature più una colonna per ogni classe possibile (**193 + 8 = 201 colonne**).\n",
        "\n",
        "Per la classe di appartenenza è stata utilizzata una rappresentazione in One Hot Encode (usando la funzione [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) di Pandas) per evitare che il modello apprenda relazioni in realtà non esistenti tra i valori della classe dei dati."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_time_series_list(augment_dataset):\n",
        "  #define time series lists (for standard ts and for augmented ts)\n",
        "  ts_list = []; ts_static_noise = []; ts_shift = []; ts_stretch = []; ts_pitch = []; ts_dyn_change = []; ts_sep = []\n",
        "  #time series extraction from files\n",
        "  for _,row in files_df.iterrows():\n",
        "    time_series,_ = librosa.load(path=row['Path'])\n",
        "    ts_list.append(time_series)    \n",
        "    #populate augmented time series lists  \n",
        "    if augment_dataset:\n",
        "      ts_static_noise.append(add_static_noise(time_series))\n",
        "      ts_shift.append(add_static_noise(time_series))\n",
        "      ts_stretch.append(add_static_noise(time_series))\n",
        "      ts_pitch.append(add_static_noise(time_series))\n",
        "      ts_dyn_change.append(add_static_noise(time_series))\n",
        "      ts_sep.append(add_static_noise(time_series))\n",
        "  #merge time series lists\n",
        "  if augment_dataset:\n",
        "    ts_list = np.concatenate([ts_list, ts_static_noise, ts_shift, ts_stretch, ts_pitch, ts_dyn_change, ts_sep])\n",
        "  return ts_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_uVqBX6DLNd"
      },
      "outputs": [],
      "source": [
        "def get_feature_dataframe(augment_dataset=False, standardize=True):\n",
        "  #support arrays to store feature values\n",
        "  chroma_col = []; mel_spectrogram_col = []; mfcc_col = []; spectral_contrast_col = []; tonnetz_col = []\n",
        "\n",
        "  #features extraction from time series\n",
        "  for time_series in get_time_series_list(augment_dataset):\n",
        "    chroma, mel_spectrogram, mfcc, spectral_contrast, tonnetz = get_features(time_series)\n",
        "\n",
        "    chroma_col.append(chroma)\n",
        "    mel_spectrogram_col.append(mel_spectrogram)  \n",
        "    mfcc_col.append(mfcc)\n",
        "    spectral_contrast_col.append(spectral_contrast)\n",
        "    tonnetz_col.append(tonnetz)\n",
        "\n",
        "  #convert feature arrays to pandas dataframes and chain them them to obtain the feature dataframe\n",
        "  chroma_df = pd.DataFrame(chroma_col)\n",
        "  chroma_df.rename(columns=lambda i: 'Chroma_' + str(i+1), inplace=True)\n",
        "\n",
        "  mel_df = pd.DataFrame(mel_spectrogram_col)\n",
        "  mel_df.rename(columns=lambda i: 'MelSpectrogram_' + str(i+1), inplace=True)\n",
        "\n",
        "  mfcc_df = pd.DataFrame(mfcc_col)\n",
        "  mfcc_df.rename(columns=lambda i: 'MFCC_' + str(i+1), inplace=True)\n",
        "\n",
        "  contrast_df = pd.DataFrame(spectral_contrast_col)\n",
        "  contrast_df.rename(columns=lambda i: 'SpectralContrast_' + str(i+1), inplace=True)\n",
        "\n",
        "  tonnetz_df = pd.DataFrame(tonnetz_col)\n",
        "  tonnetz_df.rename(columns=lambda i: 'Tonnetz_' + str(i+1), inplace=True)\n",
        "\n",
        "  features_df = pd.concat([chroma_df, mel_df, mfcc_df, contrast_df, tonnetz_df], axis=1)\n",
        "\n",
        "  #standardize features\n",
        "  if standardize:\n",
        "    for column in features_df:\n",
        "      features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
        "\n",
        "  emotion_df_ohe = pd.get_dummies(data=emotion_df, prefix=\"emotion\", columns=[\"Emotion\"])\n",
        "\n",
        "  if(augment_dataset):\n",
        "    actor_df2 = pd.concat([actor_df]*7, ignore_index=True)\n",
        "    emotion_df_ohe2 = pd.concat([emotion_df_ohe]*7, ignore_index=True)\n",
        "    return pd.concat([actor_df2, features_df, emotion_df_ohe2], axis=1)\n",
        "  else:\n",
        "    return pd.concat([actor_df, features_df, emotion_df_ohe], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per valutare l'efficacia dell'augmentation del dataset sono stati condotti due training usando lo stesso modello (LSTM), il primo utilizzando il dataset standard e il secondo usando la versione estesa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQCd3CDB-JbG"
      },
      "source": [
        "### Creazione del dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfrqVTSy9DMh"
      },
      "source": [
        "Il seguente blocco di codice permette di decidere se ricreare il dataframe o se caricare la versione salvata su Drive, risparmiando il tempo necessario per la costruzione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt1p8z5BK4iE"
      },
      "outputs": [],
      "source": [
        "def get_dataframe(path):\n",
        "  if os.path.exists(path):\n",
        "    #create dataframe from CSV file\n",
        "    dataframe = pd.read_csv(path)\n",
        "  else:\n",
        "    #create dataframe from dataset\n",
        "    print(\"creating dataframe...\")\n",
        "    dataframe = get_feature_dataframe(augment_dataset=augmented_dataset)\n",
        "    print(\"dataframe creation completed!\")\n",
        "    #save dataframe ad CSV file\n",
        "    dataframe.to_csv(path, index=False) #index param avoid to save row names (unused information)\n",
        "  return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataframe = get_dataframe(dataframe_path)\n",
        "dataframe_aug = get_dataframe(dataframe_aug_path)\n",
        "print(\"standard dataframe shape: {}, augmented dataframe shape: {}\".format(dataframe.shape, dataframe_aug.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r93xB53QKk--"
      },
      "source": [
        "Viene usata la funzione [info](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) dei DataFrame Pandas per visualizzare informazioni interessanti sul DataFrame. A causa del elevato numero di colonne è necessario impostare il parametro `verbose` per forzare la visualizzazione completa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN5FlJoOH2vY",
        "outputId": "37735d11-8fd9-426e-d62c-bb3f8a8f4c4e"
      },
      "outputs": [],
      "source": [
        "dataframe.info()\n",
        "#dataframe.info(verbose=True) #used only for debug purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "MMsvFDVPLwjH",
        "outputId": "269a642d-0d68-4e15-edc9-15f15b84c4f3"
      },
      "outputs": [],
      "source": [
        "dataframe.describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmQrC3BNdSfs"
      },
      "source": [
        "### Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "420IgK8zE5yK",
        "outputId": "d08718ca-3f08-408b-a759-d6427c4ad0bd"
      },
      "outputs": [],
      "source": [
        "available_actor_indexes = dataframe['Actor'].unique()\n",
        "test_actor_indexes = [1, 12, 7, 24]\n",
        "validation_actor_indexes = [2,13,8,23]\n",
        "#subtraction with list comprehension\n",
        "training_actor_indexes = [x for x in available_actor_indexes if x not in test_actor_indexes and x not in validation_actor_indexes]\n",
        "print(\"training actors are {} over a total of {}\"\n",
        "      .format(len(training_actor_indexes),len(available_actor_indexes)))\n",
        "\n",
        "#test and validation set are sampled from not-augmented dataset\n",
        "test_set = dataframe.loc[dataframe['Actor'].isin(test_actor_indexes)]\n",
        "validation_set = dataframe.loc[dataframe['Actor'].isin(validation_actor_indexes)]\n",
        "\n",
        "#project validation set to get separately data and classes\n",
        "validation_data = validation_set.iloc[:,1:features_count+1]\n",
        "validation_classes = validation_set.iloc[:,features_count+1:]\n",
        "#reshape data\n",
        "validation_data = validation_data.to_numpy()\n",
        "validation_data = validation_data.reshape((validation_data.shape[0], validation_data.shape[1], 1))\n",
        "\n",
        "#display dataset subset shape\n",
        "print(\"Shapes -> validation set: {}, test set: {}\"\n",
        ".format(validation_set.shape, test_set.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu5wDTCVVIlv"
      },
      "source": [
        "## Funzione di training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQAY38QUGI8D"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model, iterations=1):\n",
        "  #define array of metrics, one value for every cross-validation iteration\n",
        "  model_loss = []; model_accuracy = []\n",
        "\n",
        "  #on every iteration fix a different actor, train on the other actors and test on him\n",
        "  for iteration in range(iterations):\n",
        "    fold_index = 0 #used to print the index of the current processing fold\n",
        "\n",
        "    for test_actor_index in training_actor_indexes:\n",
        "      \n",
        "      cross_train_actor_indexes = [x for x in training_actor_indexes if x != test_actor_index]\n",
        "      #define train and test set for the current iteration (training set created from augmented dataframe)\n",
        "      cross_test_set = dataframe.loc[dataframe['Actor'].isin([test_actor_index])]\n",
        "      cross_training_set = dataframe_aug.loc[dataframe_aug['Actor'].isin(cross_train_actor_indexes)]      \n",
        "      #project training set and test set to get separately data and classes\n",
        "      train_data = cross_training_set.iloc[:,1:features_count+1]\n",
        "      train_classes = cross_training_set.iloc[:,features_count+1:]\n",
        "      test_data = cross_test_set.iloc[:,1:features_count+1]\n",
        "      test_classes = cross_test_set.iloc[:,features_count+1:]\n",
        "      #reshape data\n",
        "      train_data = train_data.to_numpy()\n",
        "      train_data = train_data.reshape((train_data.shape[0], train_data.shape[1], 1))\n",
        "      test_data = test_data.to_numpy()\n",
        "      test_data = test_data.reshape((test_data.shape[0], test_data.shape[1], 1))\n",
        "      # fit the classifier\n",
        "      fold_index += 1\n",
        "      history = model.fit(x=train_data, y=train_classes, verbose=0)\n",
        "      # compute the score and record it\n",
        "      test_result = model.evaluate(x=test_data, y=test_classes, verbose=0)\n",
        "      model_loss.append(test_result[0]) #metrics are loss and accuracy\n",
        "      model_accuracy.append(test_result[1])\n",
        "\n",
        "      print(\"Iter {}/{}, Fold {}/{}, {} data, fit loss: {:.4f}, fit accuracy: {:.4f}, test loss: {:.4f}, test accuracy {:.4f}\".format(\n",
        "          iteration+1, iterations, fold_index, len(training_actor_indexes), train_data.shape[0],\n",
        "          history.history['loss'][0], history.history['accuracy'][0], test_result[0], test_result[1]))\n",
        "\n",
        "  return model_loss, model_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxKm3eAZCxvC"
      },
      "source": [
        "\n",
        "## RNN base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWAPEnPrFRHJ"
      },
      "source": [
        "Il primo esperiemnto viene condotto usando una RNN many-to-one standard, cioè con un unico nodo. \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RNN/ManyToOne_RNN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAtLb4wSC0aS"
      },
      "source": [
        "### Definizione del modello"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t87ASrwsMtRs"
      },
      "source": [
        "[SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSR8XlZvC6Rz"
      },
      "outputs": [],
      "source": [
        "def build_simple_rnn(n_feature,n_classes):\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "        Input(shape=(n_feature,1)),\n",
        "        #SimpleRNN(n_classes, activation=\"softmax\")\n",
        "        \n",
        "        SimpleRNN(8, activation=\"relu\"),\n",
        "        Dense(n_classes, activation=\"softmax\")\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePsWiMB8C6az"
      },
      "source": [
        "### Creazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FortBIefC9aq"
      },
      "outputs": [],
      "source": [
        "simple_rnn = build_simple_rnn(features_count, classes_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNBb9hS6C_JK"
      },
      "source": [
        "### Visualizzazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIzyYeNLKme6"
      },
      "outputs": [],
      "source": [
        "simple_rnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "147xnyRvKnFK"
      },
      "outputs": [],
      "source": [
        "plot_model(simple_rnn, show_shapes=True, show_layer_names=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCWiM7XDCvS"
      },
      "source": [
        "### Compilazione del modello"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_JhFrXUbUHs"
      },
      "source": [
        "[Adam](https://keras.io/api/optimizers/adam/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrWJN9I9hYtE"
      },
      "outputs": [],
      "source": [
        "loss = BinaryCrossentropy()\n",
        "optimizer = Adam()\n",
        "\n",
        "simple_rnn.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPniNsZ5DGga"
      },
      "source": [
        "### Training del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhypJiFyM3Ij"
      },
      "outputs": [],
      "source": [
        "#loss, accuracy = cross_validation(simple_rnn,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLnGnDlMqD9Z"
      },
      "outputs": [],
      "source": [
        "#plot_history(loss, accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RCF1vFy8Ji7"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufiXdwlWcpxD"
      },
      "source": [
        "### Definizione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lcMAkz9xmgx"
      },
      "outputs": [],
      "source": [
        "def build_lstm(n_feature,n_classes):\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "        Input(shape=(n_feature,1)),\n",
        "        LSTM(8, activation=\"relu\"),\n",
        "        Dense(n_classes, activation=\"softmax\")\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CcQJBshzntn"
      },
      "source": [
        "### Creazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QK_x7Rcznto"
      },
      "outputs": [],
      "source": [
        "lstm = build_lstm(features_count, classes_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Evpj6-znto"
      },
      "source": [
        "### Visualizzazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFEh_zDnzntp"
      },
      "outputs": [],
      "source": [
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac5euvv4zntq"
      },
      "outputs": [],
      "source": [
        "plot_model(lstm, show_shapes=True, show_layer_names=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzytnyF-zntq"
      },
      "source": [
        "### Compilazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLHGHtRPzntr"
      },
      "outputs": [],
      "source": [
        "loss = BinaryCrossentropy()\n",
        "optimizer = Adam()\n",
        "\n",
        "lstm.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vdpV47ezntr"
      },
      "source": [
        "### Training del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWFVZLxGznts"
      },
      "outputs": [],
      "source": [
        "#loss, accuracy = cross_validation(lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIFRT5KoqvnG"
      },
      "outputs": [],
      "source": [
        "#plot_history(loss, accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qY9c5cdmHHjD"
      },
      "source": [
        "## Advanced Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0LK38gKWyA"
      },
      "source": [
        "### Definizione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME52aw5mFxUt"
      },
      "outputs": [],
      "source": [
        "def build_model(n_feature,n_classes):\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "        Input(shape=(n_feature,1)),\n",
        "        Conv1D(32, 8, padding=\"same\"),\n",
        "        BatchNormalization(axis=-1),\n",
        "        Activation(\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        MaxPool1D(),\n",
        "        Conv1D(64, 6, padding=\"same\"),\n",
        "        BatchNormalization(axis=-1),\n",
        "        Activation(\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        MaxPool1D(),\n",
        "        Conv1D(128, 4, padding=\"same\"),\n",
        "        BatchNormalization(axis=-1),\n",
        "        Activation(\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        MaxPool1D(),  \n",
        "        Flatten(),\n",
        "        Dense(128),\n",
        "        BatchNormalization(axis=-1),\n",
        "        Activation(\"relu\"),\n",
        "        Dense(n_classes, activation=\"softmax\")\n",
        "      ]\n",
        "    )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNihiZKAGQGk"
      },
      "source": [
        "### Creazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jAhTDgHGQGl"
      },
      "outputs": [],
      "source": [
        "model = build_model(features_count, classes_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ahiZwVGQGl"
      },
      "source": [
        "### Visualizzazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urFzJCCcGQGl",
        "outputId": "04c3a17b-45db-4874-ee02-f4fc766ad4f7"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "5eLNM3aKGQGl",
        "outputId": "00388015-a4ff-419d-f358-5795fae62404"
      },
      "outputs": [],
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_-1IGRPGQGl"
      },
      "source": [
        "### Compilazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lgy9HdLGQGm"
      },
      "outputs": [],
      "source": [
        "loss = BinaryCrossentropy()\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU3q7LdWGQGm"
      },
      "source": [
        "### Training del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdV83IFSGQGm",
        "outputId": "612663ac-84d9-470a-b95b-2c8b67f3d916"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = cross_validation(model,20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "QUtgyJDLGQGm",
        "outputId": "8838abd0-5a10-4e47-df1f-48748dae0d56"
      },
      "outputs": [],
      "source": [
        "plot_history(loss, accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk4fFRmFfmX4"
      },
      "source": [
        "## Risultati del training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONADU9UpgmrH",
        "outputId": "e230cb90-0594-485e-8ce9-99087f746fb7"
      },
      "outputs": [],
      "source": [
        "#simple_rnn.evaluate(validation_data, validation_classes)\n",
        "#lstm.evaluate(validation_data, validation_classes)\n",
        "model.evaluate(validation_data, validation_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#LISTA DI RISULTATI\n",
        "# acc: 0.3292   3x lstm, neuroni: (32,32,32) , 10x CV\n",
        "# acc: 0.3542   1x lstm, neuroni\n",
        "# acc: 0.3333   3x lstm, neuroni (8,8,8), 10x CV"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "f154974daf592dbeb4bdb9aa984a25b1c8bbfc08b696a25066637c75bdc145f5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
